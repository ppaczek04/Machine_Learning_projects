# -*- coding: utf-8 -*-
"""lab09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gktz8JaZKiNKraEpmqSx4DzK0ZtUcZCw

# Data classification and regression in Keras Tensorflow

# Neural Network Experiments with Keras and TensorFlow

In this notebook, we experiment with artificial neural networks using the Keras API (within TensorFlow).  
The notebook is divided into two main parts:

1. **Image Classification** using the Fashion MNIST dataset.
2. **Regression** using the California Housing dataset.

We demonstrate:
- Data preprocessing and normalization
- Building and compiling models with the Sequential API
- Training models with callbacks such as TensorBoard and EarlyStopping
- Saving trained models to files
- Comparing different architectures through experiments and visualizations

This notebook serves as a hands-on lab session exploring how neural networks are applied to classification and regression tasks.

## 1. Data import

**if needed we install tensorflow framework**
"""

#!pip install -U tensorflow

import tensorflow as tf
from tensorflow import keras

tf.__version__

"""## 1. Data classification - images

**We import the popular cloth MNIST dataset, not the digits, but clothes this time, so the items in the same class are more diverse. It will be a challange for our neural network**
"""

fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

X_train[100][0]

"""### Scaling Pixel Values to [0, 1]

The original pixel values in the Fashion MNIST dataset range from **0 to 255**.  
To make the training process more efficient, we **normalize** the data by dividing all values by 255, scaling them to the range **[0, 1]**.

**Why is this important?**
- Neural networks train more effectively when input values are on a similar and smaller scale.
- Normalization improves **convergence speed** and **stability** during training.
- It helps prevent problems like exploding gradients in deeper networks.
"""

X_train = X_train / 255
X_test = X_test / 255

X_train[100][0]

"""**we show an example image form MNIST dataset**"""

import matplotlib.pyplot as plt
plt.figure(figsize = (2,2))
plt.imshow(X_train[42], cmap="binary")
plt.axis('off')
plt.show()

class_names = [
    "T-shirt/top",   # koszulka
    "Trouser",       # spodnie
    "Pullover",      # pulower
    "Dress",         # sukienka
    "Coat",          # kurtka
    "Sandal",        # sandał
    "Shirt",         # koszula
    "Sneaker",       # półbut (sportowy)
    "Bag",           # torba
    "Ankle boot"     # but (najczęściej botek)
]

"""**Labels of images are in scale from 0 to 9, but we create a list with names of class so its easier for us to see what class-image is being depicted (name on the right in the comment is a name of the class in a polish - my native language)**"""

class_names[y_train[42]]

"""### Neural Network Architecture for Image Classification

In this section, we define a **Sequential** neural network model using Keras, designed for multiclass image classification (Fashion MNIST dataset).  
The architecture consists of the following layers:

1. **Flatten Layer**  
   - `keras.layers.Flatten(input_shape=[28, 28])`  
   - Converts each 28×28 image (2D) into a flat vector of 784 values (1D).
   - This is necessary because dense layers expect one-dimensional input.

2. **First Dense Layer**  
   - `keras.layers.Dense(300, activation="relu")`  
   - Fully connected layer with **300 neurons**.
   - Uses **ReLU (Rectified Linear Unit)** activation, which introduces non-linearity into the model.

3. **Second Dense Layer**  
   - `keras.layers.Dense(100, activation="relu")`  
   - Fully connected layer with **100 neurons**, also using ReLU activation.

4. **Output Layer**  
   - `keras.layers.Dense(10, activation="softmax")`  
   - Output layer with **10 neurons**, one for each class in the Fashion MNIST dataset.
   - The **softmax** activation function transforms the output into a probability distribution:
     - Each output value is in the range (0, 1)
     - The sum of all output values equals **1**
     - This allows the model to express confidence for each class and makes it suitable for multiclass classification.

This type of architecture is common and effective for tasks involving image data and categorical outputs.
"""

model_clf = keras.models.Sequential()
model_clf.add(keras.layers.Flatten(input_shape=[28, 28]))
model_clf.add(keras.layers.Dense(300, activation="relu"))
model_clf.add(keras.layers.Dense(100, activation="relu"))
model_clf.add(keras.layers.Dense(10, activation="softmax"))

model_clf.summary()

"""### Compiling the Model

We compile the model by specifying:
- **Loss function**: `sparse_categorical_crossentropy` – used for multiclass classification with integer class labels.
- **Optimizer**: `sgd` (Stochastic Gradient Descent) – standard gradient-based optimizer.
- **Metric**: `"accuracy"` – allows us to monitor classification accuracy during training and validation.
"""

model_clf.compile(loss="sparse_categorical_crossentropy",
 optimizer="sgd",
 metrics=["accuracy"])

"""### Preparing TensorBoard Logging

We set up a directory for TensorBoard logs:
- The base directory is called `image_logs`
- Each training run gets its own subdirectory with a unique timestamp (e.g. `run_2025_05_15-14_22_10`)
- A `TensorBoard` callback is created to log training metrics into this directory, allowing us to visualize them later.
"""

import os
root_logdir = os.path.join(os.curdir, "image_logs")
def get_run_logdir():
  import time
  run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
  return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()

tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)

"""### Training the Model with Validation and TensorBoard

We train the model using:
- `epochs=20`: maximum of 20 training cycles
- `validation_split=0.1`: 10% of the training data is automatically reserved for validation
- `callbacks=[tensorboard_cb]`: TensorBoard is used to log training metrics for real-time monitoring

This setup allows us to observe training/validation loss and accuracy trends over epochs.
"""

history = model_clf.fit(X_train, y_train, epochs=20,
                     validation_split=0.1   # <-- this reserves 10% of the training data as validation data,
                      #     which is used during training (between epochs)
                      #     to evaluate the model's performance on unseen data
                      #     and monitor overfitting
                    , callbacks = [tensorboard_cb])

"""### Launching TensorBoard and Saving the Model

- `%load_ext tensorboard`  
  Loads the TensorBoard extension inside the Jupyter environment, allowing in-notebook visualizations.  
  If already loaded, it can be refreshed using `%reload_ext tensorboard`.

- `%tensorboard --logdir=image_logs --port=6006`  
  Starts TensorBoard on port 6006 using the log files stored in the `image_logs` directory.  
  This enables real-time monitoring of training metrics such as loss and accuracy.

- `model_clf.save('fashion_clf.keras')`  
  Saves the trained classification model to a file named `fashion_clf.keras`, including:
  - model architecture
  - learned weights
  - training configuration (loss, optimizer, metrics)

  > **Note:** If you get a "Not found" message in TensorBoard, it may mean no valid logs were written, or it reused a stale instance. Restarting the notebook kernel or killing the old process with `!kill [PID]` can help.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=image_logs --port=6006

model_clf.save('fashion_clf.keras')

"""### Interpreting the Accuracy Plot

From the displayed accuracy plot:

- The **training accuracy** (green line) reaches approximately **90.1%**, indicating that the model is learning the training data very well.
- The **validation accuracy** (orange line) levels off around **88.4%**, which is also a strong result. It shows that the model generalizes well to unseen data.
- Both curves show a steady upward trend, and the gap between them is small.

✅ This suggests that the model is **not overfitting** – it performs well on both training and validation data, which is ideal for a classification task.

### Making Predictions on Test Samples

We randomly select an image from the test set and make a prediction using the trained model:
- `np.random.randint(...)` chooses a random index
- The image is reshaped into a batch before prediction
- `model_clf.predict(...)` returns class probabilities
- `np.argmax(...)` gives the predicted class
- We also extract the **confidence** of the prediction (highest probability)

Finally, we print the predicted class and its confidence score.
"""

import numpy as np


image_index = np.random.randint(len(X_test))
image = np.array([X_test[image_index]])
confidences = model_clf.predict(image)
confidence = np.max(confidences[0])
prediction = np.argmax(confidences[0])
print("Prediction:", class_names[prediction])
print("Confidence:", confidence)

"""### Displaying the Prediction Result

We compare the model's prediction with the **true label** of the selected image:
- The ground truth class is printed
- The corresponding test image is displayed in grayscale (`cmap="binary"`)
- Axes are hidden for a cleaner visualization

This helps visually verify if the model's prediction matches the actual image content.
"""

print("Truth:", class_names[y_test[image_index]])
plt.figure(figsize = (2,2))
plt.imshow(image[0], cmap="binary")
plt.axis('off')
plt.show()

"""## 3. Data regression - iris dataset

### Logging Directory Setup for TensorBoard

We define a logging directory for saving training metrics for TensorBoard visualization.  
Each run gets a unique subdirectory based on the current timestamp, stored under the `house_logs` root folder
"""

import os
root_logdir = os.path.join(os.curdir, "house_logs")
def get_run_logdir():
  import time
  run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
  return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir()

"""### Feature Normalization

We add a **Normalization layer** to standardize input features:
- Normalization ensures each input feature has zero mean and unit variance.
- This helps the model converge faster and more stably.
- `normalizer.adapt(X_train_full)` calculates the required statistics (mean and variance) based on the training set.

###  Building the Regression Model

We define a **Sequential model** with the following architecture:
- **Normalization layer** (first)
- **Three Dense layers**, each with 50 neurons and ReLU activation
- **Output layer** with 1 neuron (no activation) for predicting a continuous value (house price)

This design is suitable for **regression** tasks where the goal is to predict a single numeric output.
"""

from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

housing = fetch_california_housing()
X, y = housing.data, housing.target


# 1. We split data: 70% trainging data, 15% validation data, 15% test data
X_train_full, X_temp, y_train_full, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# 2. Normalisation of training data
normalizer = keras.layers.Normalization(input_shape=X.shape[1:], axis=-1)
normalizer.adapt(X_train_full)

# 3. We create a model
model = keras.models.Sequential([
    normalizer,
    keras.layers.Dense(50, activation="relu"),
    keras.layers.Dense(50, activation="relu"),
    keras.layers.Dense(50, activation="relu"),
    keras.layers.Dense(1)
])

# 4. We compile this model
model.compile(
    loss="mse",
    optimizer="adam",
    metrics=[keras.metrics.RootMeanSquaredError()]
)

"""### Training the Model with Callbacks

We train the model using the training set (`X_train_full`, `y_train_full`) and evaluate it on the validation set after each epoch:

- `epochs=100`: allows for up to 100 training iterations.
- `validation_data`: used to monitor generalization performance.
- `callbacks`: we use two:
  - `TensorBoard` – for real-time training visualization.
  - `EarlyStopping` – stops training early if the validation loss does not improve by at least `0.01` for `5` consecutive epochs.

This helps prevent overfitting and saves time by avoiding unnecessary training.

### Saving the Trained Model

We save the trained regression model to a file named **`reg_housing_1.keras`**.  
This file contains the full model, including:
- architecture
- learned weights
- optimizer state
- compilation configuration

It can be reloaded later for evaluation or inference.
"""

run_logdir = get_run_logdir()
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, min_delta=0.01, verbose=1)

# 5. We train this model
history = model.fit(
    X_train_full, y_train_full,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=[tensorboard_cb, early_stopping_cb]
)

# 6. Saving created model into the .keras file
model.save("reg_housing_1.keras")

"""### TensorBoard Setup for Visualization

We load the TensorBoard extension and launch it using logs stored in the `image_logs` directory.  
This allows us to **visualize and compare training progress** (e.g. loss, accuracy, RMSE) between different model configurations.  
TensorBoard is especially useful when experimenting with architecture variations, as it helps highlight their impact.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=image_logs --port=6006

"""### Creating and Training Another Regression Model for Comparison

In this experiment, we reuse the same architecture as in the previous test:
- Three hidden layers with 20, 50, and 100 neurons, each using **ReLU** activation.
- One output neuron (no activation) for predicting a continuous value.

The model is compiled with:
- Loss: **Mean Squared Error** (`"mse"`), standard for regression tasks.
- Optimizer: **Adam**, adaptive and efficient.
- Metric: **Root Mean Squared Error** (RMSE), easier to interpret than MSE.

We train the model for up to 100 epochs using:
- **EarlyStopping** to halt training when validation performance stops improving.
- **TensorBoard** to log training metrics for visualization and comparison.

Finally, the trained model is saved as `reg_housing_2.keras`.

🔍 This run allows us to **validate repeatability and consistency** of results when using the same architecture but a different training session.
"""

# 3A. We create a model
model = keras.models.Sequential([
    normalizer,
    keras.layers.Dense(20, activation="relu"),
    keras.layers.Dense(50, activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(1)
])

# 4A. We compile this model
model.compile(
    loss="mse",
    optimizer="adam",
    metrics=[keras.metrics.RootMeanSquaredError()]
)

# 5A. We train this model
history = model.fit(
    X_train_full, y_train_full,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=[tensorboard_cb, early_stopping_cb]
)

# 6A. Saving created model into the .keras file
model.save("reg_housing_2.keras")

"""### Creating and Training Another Regression Model for Comparison

In this experiment, we reuse the same architecture as in the previous test:
- Three hidden layers with 100, 50, and 20 neurons, each using **ReLU** activation.
- One output neuron (no activation) for predicting a continuous value.

The model is compiled with:
- Loss: **Mean Squared Error** (`"mse"`), standard for regression tasks.
- Optimizer: **Adam**, adaptive and efficient.
- Metric: **Root Mean Squared Error** (RMSE), easier to interpret than MSE.

We train the model for up to 100 epochs using:
- **EarlyStopping** to halt training when validation performance stops improving.
- **TensorBoard** to log training metrics for visualization and comparison.

Finally, the trained model is saved as `reg_housing_3.keras`.

🔍 This run allows us to **validate repeatability and consistency** of results when using the same architecture but a different training session.
"""

# 3B. We create a model
model = keras.models.Sequential([
    normalizer,
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(50, activation="relu"),
    keras.layers.Dense(20, activation="relu"),
    keras.layers.Dense(1)
])

# 4B. We compile this model
model.compile(
    loss="mse",
    optimizer="adam",
    metrics=[keras.metrics.RootMeanSquaredError()]
)

# 5B. We train this model
history = model.fit(
    X_train_full, y_train_full,
    epochs=100,
    validation_data=(X_valid, y_valid),
    callbacks=[tensorboard_cb, early_stopping_cb]
)

# 6B. Saving created model into the .keras file
model.save("reg_housing_3.keras")

"""### Comparison of Network Architectures: 100/50/20 vs 20/50/100 vs 50/50/50

We trained three neural network models with identical compilation and training settings, but different internal architectures.  
Here's a summary of their performance based on early stopping and validation RMSE:

#### 1. Architecture: 100 → 50 → 20 (descending)
- **Stopped at epoch 12**
- **Final val_RMSE ≈ 0.555**
- This architecture **converged quickly** and reached a stable validation error early on.
- It seems to generalize well and efficiently utilize model capacity.

#### 2. Architecture: 20 → 50 → 100 (ascending)
- **Stopped at epoch 19**
- **Final val_RMSE ≈ 0.559**
- This setup required **more epochs** to converge and showed slightly **less stability** in early training.
- May have underutilized early layers and overburdened later ones, which could lead to suboptimal feature extraction.

#### 3. Architecture: 50 → 50 → 50 (uniform)
- **Stopped at epoch 15**
- **Final val_RMSE ≈ 0.554**
- Very balanced performance — consistent, stable, and reliable.
- This architecture performs slightly better than 20/50/100 and on par with 100/50/20.

---

### ✅ Conclusions:
- **Shallow to deep (100→50→20)** often performs better, as deeper layers progressively learn higher-level features.
- **Uniform layers (50→50→50)** offer solid baseline performance with good convergence speed and stability.
- **Deepening late in the network (20→50→100)** can increase training time and may introduce instability or overfitting without performance gain.

Most models trained in under **20 epochs** thanks to **early stopping** — a common and effective regularization technique.
"""