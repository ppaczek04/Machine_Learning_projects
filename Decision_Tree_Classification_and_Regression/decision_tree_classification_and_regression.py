# -*- coding: utf-8 -*-
"""decision_tree_classification_and_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iSWk7BYB2slEnQGdIgA3Livby16cOQKQ

# Decision tree classification and regression

## 1. Data import
"""

from sklearn import datasets

data_breast_cancer = datasets.load_breast_cancer(as_frame=True)
print(data_breast_cancer['DESCR'])

df_data_breast_cancer = data_breast_cancer.frame
df_data_breast_cancer.head(10)

"""**In this project we will train Decision Tree Models to classify new cases of WDBC into Malignant Tumors and Benign Tumors.**

**In the LnearSVC/SVC project we used metrics "mean area", "mean smoothness", so this time we will choose "mean texture", "mean symmetry".**
"""

from sklearn.model_selection import train_test_split
X = df_data_breast_cancer.iloc[:,:-1]
y = df_data_breast_cancer['target']
X = X[["mean texture", "mean symmetry"]]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True)

"""## 2. Data classification with Decision Trees

**Decision Trees are an attractive classification model because they can adapt well to data
and capture nonlinear relationships between features and classes. They are also easy to interpret.
However, they are prone to overfitting, especially when not properly regularized.
To address this, in this section we will control model complexity using the `max_depth` parameter,
aiming to find a good balance between performance and generalization.**

**Note: There are also other hyperparameters worth exploring — such as `min_samples_leaf`,
`min_samples_split`, or `max_leaf_nodes` — which can further help in preventing overfitting.
In this project, we focus only on tuning `max_depth`, but testing these additional parameters
might lead to even better model performance.**
"""

from sklearn.tree import DecisionTreeClassifier
best_score = 0
X_graph = []
Y_graph_train = []
Y_graph_test = []
best_classification_results = []

for iter in range(2,20):
    print(f"----***{iter}***----")
    tree_clf = DecisionTreeClassifier(max_depth=iter, random_state=42)
    tree_clf.fit(X_train, y_train) # we train model on data from train part

    y_train_pred = tree_clf.predict(X_train)
    y_test_pred = tree_clf.predict(X_test)

    from sklearn.metrics import precision_score, recall_score, f1_score

    train_f1_score = f1_score(y_train_pred, y_train)
    test_f1_score = f1_score(y_test_pred, y_test)
    print(f"train f1_score: {train_f1_score} - test f1_score: {test_f1_score}")
    train_accuracy = tree_clf.score(X_train, y_train)
    test_accuracy = tree_clf.score(X_test, y_test)
    print(f"train accuracy: {train_accuracy} - test accuracy: {test_accuracy}")
    print("---------------------")
    if best_score < test_f1_score:
        best_score = test_f1_score
        best_classification_results = [iter, train_f1_score, test_f1_score, train_accuracy, test_accuracy]
    X_graph.append(iter)
    Y_graph_train.append(train_f1_score)
    Y_graph_test.append(test_f1_score)

print(best_classification_results)

"""**We create a png file with the visualization of the decision tree with paramaters guaranteeing best accuracy max(f1_test_score)**"""

tree_clf = DecisionTreeClassifier(max_depth= best_classification_results[0], random_state=42)
tree_clf.fit(X_train, y_train) # we train model on data from train part

from sklearn.tree import export_graphviz
f = "bc.dot"
export_graphviz(tree_clf, out_file=f, feature_names= list(X.columns),
                class_names=[str(num)+", "+name for num,name in zip(set(y), data_breast_cancer.target_names)],
                rounded=True, filled=True)

"""**textual version of the decision tree**"""

from sklearn import tree

string_tree = tree.export_graphviz(tree_clf)
print(string_tree)

"""**and now depiction of the visualization**"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plot_tree(tree_clf,
          feature_names=X.columns,
          class_names=data_breast_cancer.target_names,
          filled=True,
          rounded=True)
plt.savefig("bc.png")  # saving to png file
plt.show()             # show in the jupyter notebook

"""**Below we have a comparison of F1_score for train and test dataset for consequtive max_depths of the tree**"""

# Rysowanie wykresu
plt.figure(figsize=(10, 6))
plt.plot(X_graph, Y_graph_train, label="F1 Train", marker='o')
plt.plot(X_graph, Y_graph_test, label="F1 Test", marker='o')
plt.xlabel("max_depth")
plt.ylabel("F1 score")
plt.title("F1 score vs. Tree Depth")
plt.legend()
plt.grid(True)
plt.show()

"""**As shown in the plot, increasing `max_depth` leads to a continuous improvement in training F1 score,
eventually reaching perfect classification. However, the F1 score on the test set initially improves
but quickly drops and fluctuates at a lower level. This is a typical sign of overfitting — the model
becomes too specialized to the training data and loses its ability to generalize.
It confirms the importance of tuning `max_depth` to find the optimal complexity of the decision tree.**

**We save the hyperparamteters of the most accurace decision tree into pickle file**
"""

import pickle

with open('f1acc_tree.pkl', 'wb') as f:
    pickle.dump(best_classification_results, f)

"""## 3. Data regression with Decision Trees"""

import numpy as np
import pandas as pd
size = 900
X = np.random.rand(size)*5-2.5
w4, w3, w2, w1, w0 = 1, 2, 1, -4, 2
y = w4*(X**4) + w3*(X**3) + w2*(X**2) + w1*X + w0 + np.random.randn(size)*8-4
df = pd.DataFrame({'x': X, 'y': y})
df.plot.scatter(x='x',y='y')

from sklearn.tree import DecisionTreeRegressor

print(X.shape)
X = X.reshape(-1,1) # -1 meaning as many as there is [in this example (300,1)]
y = y.reshape(-1,1) # -1 meaning as many as there is [in this example (300,1)]
print(X.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True)

"""**Now we will use DecisionTreeRegressor class in order to perform regression on the data (model groups parts of data into sectors where proposed value is the avarage of all the data points int this grouped sector)**"""

from sklearn.tree import DecisionTreeClassifier
best_score = float('inf')
best_regression_results = []
for iter in range(2,20):

    tree_reg = DecisionTreeRegressor(max_depth=iter, random_state=42)
    tree_reg.fit(X_train, y_train) # we train model on data from train part

    y_train_pred = tree_reg.predict(X_train)
    y_test_pred = tree_reg.predict(X_test)

    from sklearn.metrics import mean_squared_error

    train_MSE = mean_squared_error(y_train_pred, y_train)
    test_MSE = mean_squared_error(y_test_pred, y_test)

    print(f"FOR DEPTH: {iter},     train_MSE: {train_MSE} - test_MSE: {test_MSE}")

    if best_score > 3 * test_MSE + train_MSE:
        best_score = 3 * test_MSE + train_MSE
        best_regression_results = [iter, train_MSE, test_MSE]
print(best_regression_results)

"""**depiction of the visualization of the tree with parameters providing best accuracy: min(3 * test_mse + train_mse).**  
**We want to focus on good prediction for new data, so we encourage model to focus on test_mse by multiplying test_mse by 3 in the minimalising function.**
"""

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

tree_reg = DecisionTreeRegressor(max_depth = best_regression_results[0], random_state=42)
tree_reg.fit(X_train, y_train) # we train model on data from train part

plt.figure(figsize=(12, 8))
plot_tree(tree_reg,
          feature_names=["x1"],
          filled=True,
          rounded=True)
plt.savefig("reg.png")  # saving to png file
plt.show()             # show int the jupyter notebook

"""**and now we will create a visualsation of the regression that our tree model proposed for the parameters providing best accuracy**"""

x_grid = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)
y_pred = tree_reg.predict(x_grid)

# Visualisation of the regression on data
plt.figure(figsize=(10, 6))
plt.scatter(X, y, s=10, color='blue', label='Dane')
plt.plot(x_grid, y_pred, color='red', linewidth=2, label='predicted value')
plt.xlabel("$x_1$")
plt.ylabel("$y$")
plt.title(f"Regression with Decision Tree (max_depth={best_regression_results[0]})")
plt.legend()
plt.grid(True)
#plt.savefig("reg_additional.png")  # if this file needed too
plt.show()

"""**We save the hyperparamteters of the most accurace decision tree into pickle file**"""

import pickle

with open('mse_tree.pkl', 'wb') as f:
    pickle.dump(best_regression_results, f)