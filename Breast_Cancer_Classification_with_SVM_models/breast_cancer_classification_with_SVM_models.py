# -*- coding: utf-8 -*-
"""breast_cancer_classification_with_SVM_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i-n313whNXDSggbGYSxVgv2RkvyTuyn4

# Support Vector Machine (SVM) - linear and non - linerar SVM classification

# 1. Linear classification - breast cancer recognition

**Data preparation - breast cancer wisconsin (diagnostic) dataset**
"""

from sklearn import datasets

data_breast_cancer = datasets.load_breast_cancer(as_frame=True)
print(data_breast_cancer['DESCR'])

df_data_breast_cancer = data_breast_cancer.frame
df_data_breast_cancer.head(10)

from sklearn.model_selection import train_test_split
X_bc = df_data_breast_cancer.iloc[:,:-1]
y_bc = df_data_breast_cancer['target']
X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(X_bc, y_bc, test_size=0.2, shuffle = True)

"""**We will train an SVM model to classify new cases of WDBC into Malignant Tumors and Benign Tumors.**"""

X_bc_train = X_bc_train[["mean area", "mean smoothness"]]
X_bc_test = X_bc_test[["mean area", "mean smoothness"]]

"""**Firstly, we decide to train our model without scaling the attributes, which will result in a low model accuracy, as SVM models require us to scale the attributes to similar ranges in order to be able to provide us with accurate classification**"""

import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

svm_clf = Pipeline([
    ("linear_svc", LinearSVC(C=1, loss="hinge"))
    ])
svm_clf.fit(X_bc_train, y_bc_train)

"""**We can see that the model's accuracy is relatively low due to the lack of prior feature scaling.**"""

accuracy_bc_train = svm_clf.score(X_bc_train, y_bc_train)
accuracy_bc_test = svm_clf.score(X_bc_test, y_bc_test)
print(f'train set accuracy: {accuracy_bc_train}')
print(f'test set accuracy: {accuracy_bc_test}')

"""**Now we will use StandardScaler class in order to scale attributes to similar ranges**"""

svm_clf_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svc", LinearSVC(C=1, loss="hinge")),
                             ])
svm_clf_scaled.fit(X_bc_train, y_bc_train)

"""**Now thanks to the scaling, accuracy score of our model is way higher (in the neighbourhood of 0.90)**"""

accuracy_bc_scaled_train = svm_clf_scaled.score(X_bc_train, y_bc_train)
accuracy_bc_scaled_test = svm_clf_scaled.score(X_bc_test, y_bc_test)
print(f'train scaled accuracy: {accuracy_bc_scaled_train}')
print(f'test scaled accuracy: {accuracy_bc_scaled_test}')

accuracy_score_list = [accuracy_bc_train, accuracy_bc_test, accuracy_bc_scaled_train, accuracy_bc_scaled_test]
import pickle
with open("bc_acc.pkl", "wb") as f:
    pickle.dump(accuracy_score_list, f)

"""**We created a model that classifies cancer cases as either Malignant or Benign with the accuracy of 90% thanks to the usage of LinearSVC model and attribute scaling.**

# 2. Linear classification - Iris Virginica recognition

**Data preparation - classic Iris plants dataset**
"""

data_iris = datasets.load_iris(as_frame=True)
print(data_iris['DESCR'])

df_data_iris = data_iris.frame
df_data_iris.head(10)

from sklearn.model_selection import train_test_split
X_ir = df_data_iris.iloc[:,:-1]
y_ir = df_data_iris['target']
y_ir = (y_ir == 2).astype(int)
X_ir_train, X_ir_test, y_ir_train, y_ir_test = train_test_split(X_ir, y_ir, test_size=0.2, shuffle = True)

"""**We will train SVM model to classify new cases of iris plants - whether they belong to Iris-virginica species or other species. In this dataset there are 3 types of Iris species:**
- 0 -> setosa   
- 1 -> versicolor  
- 2 -> virginica

**SVM model is a binary classificator which means it decides whether sample belongs to one class or another. Thus, later on we transform label column (target column) from inicating whether a plant is setosa/versicolor/virginica to indicating:**
- 0 -> others (setosa or versicolor)
- 1 -> virginica

**This allows us to train our model to recognize whether a sample represents the Virginica species.**
"""

X_ir_train = X_ir_train[["petal length (cm)", "petal width (cm)"]]
X_ir_test = X_ir_test[["petal length (cm)", "petal width (cm)"]]
print(y_ir)

"""**Again, firstly without scaling the attributes**"""

import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

svm_clf = Pipeline([
    ("linear_svc", LinearSVC(C=1, loss="hinge"))
                             ])
svm_clf.fit(X_ir_train, y_ir_train)

"""**no-scaling accuracy**"""

accuracy_ir_train = svm_clf.score(X_ir_train, y_ir_train)
accuracy_ir_test = svm_clf.score(X_ir_test, y_ir_test)
print(f'train set accuracy: {accuracy_ir_train}')
print(f'test set accuracy: {accuracy_ir_test}')

"""**SVM classification visualisation below**"""

import matplotlib.pyplot as plt
import numpy as np

# visualising data from dataset
plt.figure(figsize=(8,6))

# binary classification
class_0 = y_ir_train == 0
class_1 = y_ir_train == 1

plt.scatter(X_ir_train.loc[class_0, X_ir_train.columns[0]],
            X_ir_train.loc[class_0, X_ir_train.columns[1]],
            color='red', label='non-Virginica')

plt.scatter(X_ir_train.loc[class_1, X_ir_train.columns[0]],
            X_ir_train.loc[class_1, X_ir_train.columns[1]],
            color='blue', label='Virginica')

# parameters of y = ax + b division line
w = svm_clf.named_steps["linear_svc"].coef_[0]
b = svm_clf.named_steps["linear_svc"].intercept_[0]

# transformation to  y = -(w1/w2)x - (b/w2)
x_vals = np.linspace(X_ir_train.iloc[:, 0].min(), X_ir_train.iloc[:, 0].max(), 100)
y_vals = -(w[0] / w[1]) * x_vals - (b / w[1])

plt.plot(x_vals, y_vals, 'k--', label='division line')

# Description of axis/graph
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")
plt.title("SVM without scaling /*/ Virginica vs. non-Virginica")
plt.legend()
plt.grid(True)
plt.show()

"""**And now with scaling the attributes first**"""

svm_clf_scaled = Pipeline([
    ("scaler", StandardScaler()),
    ("linear_svc", LinearSVC(C=1, loss="hinge")),
                             ])
svm_clf_scaled.fit(X_ir_train, y_ir_train)

"""**accuracy score of the model with beforehand scaling**"""

accuracy_ir_scaled_train = svm_clf_scaled.score(X_ir_train, y_ir_train)
accuracy_ir_scaled_test = svm_clf_scaled.score(X_ir_test, y_ir_test)
print(f'train scaled set accuracy: {accuracy_ir_scaled_train}')
print(f'test scaled set accuracy: {accuracy_ir_scaled_test}')

accuracy_score_list = [accuracy_ir_train, accuracy_ir_test, accuracy_ir_scaled_train, accuracy_ir_scaled_test]
import pickle

with open("iris_acc.pkl", "wb") as f:
    pickle.dump(accuracy_score_list, f)

"""**SVM classification visualisation below**"""

import pandas as pd

# scaling the data manually (in order to have them scaled on visualisation)
scaler = svm_clf_scaled.named_steps["scaler"]
X_ir_train_scaled = pd.DataFrame(
    scaler.transform(X_ir_train),
    columns=X_ir_train.columns,
    index=X_ir_train.index  # zachowaj index dla maski y
)

# binary classification
class_0 = y_ir_train == 0
class_1 = y_ir_train == 1

# visualising data from dataset
plt.figure(figsize=(8,6))
plt.scatter(X_ir_train_scaled.loc[class_0, "petal length (cm)"],
            X_ir_train_scaled.loc[class_0, "petal width (cm)"],
            color='red', label='non-Virginica')

plt.scatter(X_ir_train_scaled.loc[class_1, "petal length (cm)"],
            X_ir_train_scaled.loc[class_1, "petal width (cm)"],
            color='blue', label='Virginica')

# parameters of y = ax + b division line
w = svm_clf_scaled.named_steps["linear_svc"].coef_[0]
b = svm_clf_scaled.named_steps["linear_svc"].intercept_[0]

x_vals = np.linspace(X_ir_train_scaled.iloc[:, 0].min(), X_ir_train_scaled.iloc[:, 0].max(), 100)
y_vals = -(w[0] / w[1]) * x_vals - (b / w[1])

plt.plot(x_vals, y_vals, 'k--', label='division line')

# Description of axis/graph
plt.xlabel("petal length (scaled)")
plt.ylabel("petal width (scaled)")
plt.title("SVM with scaling /*/ Virginica vs. non-Virginica")
plt.legend()
plt.grid(True)
plt.show()

"""**In this case, accuracy of the model with scaling is lower than without scaling, why is that? It's likely that petal length and petal width are naturally well-corelated and have similar range, so forcing scalling might slightly lower the model accuracy. Nevertheless, often occuring increase in model accuracy makes it worth testing/checking if scalling will help, despite some cases when it doesnt change anything.**

# BONUS: Linear regression with SVM models (SVR, LinearSVR)

**Now we will test the ability of SVR models (counterpart od SVM models but used for regression) for regression**
"""

import numpy as np
import pandas as pd
size = 900
X = np.random.rand(size)*5-2.5
w4, w3, w2, w1, w0 = 1, 2, 1, -4, 2
y = w4*(X**4) + w3*(X**3) + w2*(X**2) + w1*X + w0 + np.random.randn(size)*8-4
df = pd.DataFrame({'x': X, 'y': y})
df.plot.scatter(x='x',y='y')

print(X.shape)
X = X.reshape(-1,1) # -1 meaning as many as there is [in this example (300,1)]
y = y.reshape(-1,1) # -1 meaning as many as there is [in this example (300,1)]
print(X.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True)

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

"""**We train LinearSVR model with data transformed into fourth power by using PolynomialFeautres**"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import LinearSVR

svm_reg = Pipeline([
    ('poly', PolynomialFeatures(degree=4)),
    #('scaler', StandardScaler()),
    ('lin_svr', LinearSVR())
])

svm_reg.fit(X_train, y_train)

y_train_pred = svm_reg.predict(X_train)
y_test_pred = svm_reg.predict(X_test)

"""**We calculate Mean Squarred Error (MSE) as a measure of accuracy of our model**"""

from sklearn.metrics import mean_squared_error

train_mse = mean_squared_error(y_train, y_train_pred)
print(f'Training Mean Squared Error (MSE): {train_mse}')
test_mse = mean_squared_error(y_test, y_test_pred)
print(f'Testing Mean Squared Error (MSE): {test_mse}')

"""**Now we will use SVR model instead of LinearSVR model (Note: LinearSVR/LinearSVC scales linearly with the size of the training set, while SVR/SVC slows down significantly as the dataset grows).**"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR

svm_poly_reg = Pipeline([
    #('poly', PolynomialFeatures(degree=4)), # not needed
    # ('scaler', StandardScaler()), # not necessary elo
    ('lin_svr', SVR(kernel="poly", degree=4))
])

svm_poly_reg.fit(X_train, y_train)

y_train_pred = svm_poly_reg.predict(X_train)
y_test_pred = svm_poly_reg.predict(X_test)

"""**The reason why SVR has a higher mean squared error (MSE) than LinearSVR is due to how the models are applied to the data. In this task, the target variable is generated using a 4th-degree polynomial function. LinearSVR is used together with polynomial feature expansion (degree 4), which means the model is able to perfectly match the structure of the data. As a result, LinearSVR achieves low MSE and very accurate predictions.**

**On the other hand, SVR by default uses the RBF (radial basis function) kernel, which does not assume any specific structure like a polynomial. This makes the model less suited for fitting a function that clearly follows a polynomial pattern. Additionally, the default hyperparameters of SVR may cause either overfitting or underfitting, depending on the dataset. This leads to worse generalization and a higher MSE.**

**To improve the performance of SVR, a polynomial kernel with degree 4 should be used, and hyperparameters like C, epsilon, and coef0 should be optimized using grid search.**
"""

from sklearn.metrics import mean_squared_error

train_mse = mean_squared_error(y_train, y_train_pred)
print(f'Training Mean Squared Error (MSE): {train_mse}')
test_mse = mean_squared_error(y_test, y_test_pred)
print(f'Testing Mean Squared Error (MSE): {test_mse}')

"""**visualisation of the predictions of both models below**"""

import matplotlib.pyplot as plt

x_line = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)
y_line1 = svm_reg.predict(x_line)
y_line2 = svm_poly_reg.predict(x_line)

plt.figure(figsize=(10,5))
plt.scatter(X, y, alpha=0.3, label="data")
plt.plot(x_line, y_line1, label="LinearSVR + poly", color="green")
plt.plot(x_line, y_line2, label="SVR (kernel=poly)", color="red")
plt.legend()
plt.title("comparison of regression SVM models (SVR)")
plt.xlabel("x")
plt.ylabel("y")
plt.grid(True)
plt.show()

"""**Now we will use GridSearch method to find parameters' set that provides best accuracy**"""

from sklearn.model_selection import GridSearchCV

svm_reg_grid = SVR()

param_grid = {
"C" : [0.1, 1, 10],
"coef0" : [0.1, 1, 10]
}
search = GridSearchCV(svm_reg_grid, param_grid, scoring="neg_mean_squared_error", n_jobs=-1)
search.fit(X, y)
print(f"Cross-validated accuracy = {search.best_score_}")
print(search.best_params_)

"""**We use the best params to train our SVR model once again, and as a reuslt we get much better MSE for our SVR model**"""

svm_reg_grid_modified = SVR(C = search.best_params_["C"], coef0 = search.best_params_["coef0"] )

svm_reg_grid_modified.fit(X_train, y_train)

y_train_pred = svm_reg.predict(X_train)
y_test_pred = svm_reg.predict(X_test)

from sklearn.metrics import mean_squared_error

svrGrid_train_mse = mean_squared_error(y_train, y_train_pred)
print(f'Training Mean Squared Error (MSE): {svrGrid_train_mse}')
svrGrid_test_mse = mean_squared_error(y_test, y_test_pred)
print(f'Testing Mean Squared Error (MSE): {svrGrid_test_mse}')

"""**As you can see, SVM models which try to find best division line can be used to perform linear and non-linear regression. These models use different hyperparameters which can be manipulated in order to achieve better results with our data.**"""

import pickle

accuracy_score_list = [accuracy_bc_train, accuracy_bc_test, svrGrid_train_mse, svrGrid_test_mse]

with open("reg_mse.pkl", "wb") as f:
    pickle.dump(accuracy_score_list, f)