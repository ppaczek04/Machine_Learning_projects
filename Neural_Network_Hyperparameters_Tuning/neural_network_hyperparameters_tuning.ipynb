{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLkZ-IQG5eP3"
      },
      "source": [
        "# Hyperparameter tuning in Keras Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RimGpzdi9X7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e758f810-ae6d-415b-95be-ebd8b43c31c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.19.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#!pip install -U tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAf-C-P2LsJD"
      },
      "source": [
        "# Hyperparameter Tuning and Model Testing\n",
        "\n",
        "In this notebook, we will explore the process of tuning machine learning models, specifically neural networks, by experimenting with various hyperparameters. We will examine how different settings such as the number of hidden layers, neurons per layer, learning rate, and optimization algorithms impact model performance.\n",
        "\n",
        "The goal is to gain practical experience with both manual and automated hyperparameter optimization techniques, using tools like `RandomizedSearchCV` and `Keras Tuner`. Through this, we aim to build more accurate and stable models for predicting housing prices based on the California Housing dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdkMQ-yJ5eoP"
      },
      "source": [
        "## 1. Data import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6SCrNMcLL-8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06bb3896-41d3-4b1c-d8fe-1f9c496119c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.5.2 scikeras --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kv5A5ifL-x0"
      },
      "source": [
        "Data is split into training ~60%, validation ~20%, and test ~20% sets without a fixed random seed, so splits may vary each run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "j1ZWJeMXP3YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33497098-74c9-4ba6-a751-abd51958ee2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "numpy version: 2.0.2\n",
            "scipy version: 1.15.3\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "import numpy\n",
        "import scipy\n",
        "\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "print(\"numpy version:\", numpy.__version__)\n",
        "print(\"scipy version:\", scipy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cvtItXJa5Rc_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "a170408c-dfc5-4eff-d9fc-97ca68e32154"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_IS_PYPY' from 'sklearn.utils.fixes' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-71d8d7d7530c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhousing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_california_housing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# no random_state=42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_olivetti_faces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_olivetti_faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_openml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_rcv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m from ._samples_generator import (\n\u001b[1;32m     26\u001b[0m     \u001b[0mmake_biclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_rcv1.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_home\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRemoteFileMetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fetch_remote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pkl_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_descr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_svmlight_format_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_svmlight_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# The original vectorized data can be found at:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_svmlight_format_io.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHasMethods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStrOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_PYPY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_IS_PYPY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_IS_PYPY' from 'sklearn.utils.fixes' (/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target) # no random_state=42\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full) # no random_state=42\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GIaf6tHMPO1"
      },
      "source": [
        "Here, we create a parameter grid for hyperparameter tuning. This defines the ranges and options for the number of hidden layers, neurons per layer, learning rate, and optimizer to be explored by the grid search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un-Bzovf8kt7"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_distribs = {\n",
        "\"model__n_hidden\": [0, 1, 2, 3],\n",
        "\"model__n_neurons\": np.arange(1, 100),\n",
        "\"model__learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "\"model__optimizer\": [\"adam\", \"sgd\", \"nesterov\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E1wbF_9MiGL"
      },
      "source": [
        "This function builds a neural network model based on the given hyperparameters:\n",
        "\n",
        "- `n_hidden`: number of hidden layers,\n",
        "- `n_neurons`: number of neurons in each hidden layer,\n",
        "- `optimizer`: choice of optimizer algorithm (`sgd`, `nesterov`, `momentum`, or `adam`),\n",
        "- `learning_rate`: learning rate for the optimizer.\n",
        "\n",
        "The model uses ReLU activation in hidden layers and a single output neuron for regression (predicting house prices). The optimizer is configured according to the selected type and learning rate. The model is compiled with mean squared error (MSE) loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTHSn3wE8ng1"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden, n_neurons, optimizer, learning_rate):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(keras.layers.InputLayer(input_shape=[8]))\n",
        "  for layer in range(n_hidden):\n",
        "    model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "  model.add(keras.layers.Dense(1)) # because we estmate the price of a house so the output is one variable/value.\n",
        "\n",
        "   # oplimalizator choice\n",
        "  if optimizer == \"sgd\":\n",
        "      opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  elif optimizer == \"nesterov\":\n",
        "      opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "  elif optimizer == \"momentum\":\n",
        "      opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "  elif optimizer == \"adam\":\n",
        "      opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  else:\n",
        "      raise ValueError(f\"Nieznany optymalizator: {optimizer}\")\n",
        "\n",
        "\n",
        "\n",
        "  model.compile(loss=\"mse\", optimizer=opt)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqsn0wsvMrC_"
      },
      "source": [
        "We create a TensorBoard callback to log training progress and metrics. This allows us to visualize the learning process in TensorBoard, making it easier to monitor model performance over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk3k3QxQ9oyk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, \"house_logs\")\n",
        "def get_run_logdir():\n",
        "  import time\n",
        "  run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "  return os.path.join(root_logdir, run_id)\n",
        "\n",
        "run_logdir = get_run_logdir()\n",
        "\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create and train a test model with 1 hidden layer of 100 neurons, using the Adam optimizer and a learning rate of 0.0003. Training runs for 20 epochs, and progress is logged with TensorBoard for visualization."
      ],
      "metadata": {
        "id": "7Upi0JLMYgIN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T12fkcD-SLd"
      },
      "outputs": [],
      "source": [
        "model_clf = build_model(1,100,\"adam\", 3e-4)\n",
        "history = model_clf.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks = [tensorboard_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sLuP_J5-rkR"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjRmthuk-tJo"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=house_logs --port=6006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wO1JyfPM5pO"
      },
      "source": [
        "In this section, we train multiple models with different learning rates to observe their effect on training loss. We fix the number of hidden layers and neurons, vary the learning rate over several orders of magnitude, and plot the training loss curves on a logarithmic scale for comparison.\n",
        "\n",
        "\n",
        "**Conclusions:**  \n",
        "Increasing the learning rate generally speeds up training and leads to faster loss reduction. However, beyond a certain point, too high a learning rate causes instability in training, resulting in fluctuating or diverging loss values. Therefore, it is important to choose a learning rate that balances convergence speed and training stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqMOEN88E5Kz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rates = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "h = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = build_model(n_hidden=1, n_neurons=100, optimizer=\"adam\", learning_rate=lr)\n",
        "    history = model.fit(X_train, y_train, epochs=40,\n",
        "                        validation_data=(X_valid, y_valid),\n",
        "                        verbose=0)\n",
        "    h.append(history)\n",
        "\n",
        "# visualization\n",
        "for i, h_i in enumerate(h):\n",
        "    plt.plot(h_i.history['loss'], label=f\"Training loss 1e-{6 - i}\")\n",
        "\n",
        "plt.xlabel(\"Epoka\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title(\"Porównanie strat dla różnych learning_rate\")\n",
        "\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCGaSAFvLRAh"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4EmLM6iN_xQ"
      },
      "source": [
        "In this part, we wrap our Keras model with `KerasRegressor` to make it compatible with scikit-learn tools. We set up an early stopping callback to prevent overfitting by stopping training if the validation loss does not improve for 10 epochs.\n",
        "\n",
        "Then, we use `RandomizedSearchCV` to perform a randomized hyperparameter search over the predefined parameter grid. The search runs 5 iterations with 3-fold cross-validation, training each candidate model for up to 100 epochs with validation data.\n",
        "\n",
        "This automated search helps us find the best combination of hyperparameters to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thSl1xPVPiWP"
      },
      "outputs": [],
      "source": [
        "import scikeras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
        "\n",
        "keras_reg = KerasRegressor( model=build_model,\n",
        "    model__n_hidden=1,\n",
        "    model__n_neurons=30,\n",
        "    model__optimizer=\"adam\",\n",
        "    model__learning_rate=0.001,\n",
        "    callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZArfLRI6L2V4"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn==1.5.2 scikeras --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_m8TKMmOZor"
      },
      "source": [
        "Here, we run the randomized hyperparameter search using `RandomizedSearchCV` with 5 iterations and 3-fold cross-validation. Each model is trained for up to 100 epochs with validation data used for early stopping and performance monitoring.\n",
        "\n",
        "After the search completes, we save the best-found hyperparameters to a file (`rnd_search_params.pkl`) as well as the entire search object (`rnd_search_scikeras.pkl`) for later use or analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqEHU3bGQEL8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs,n_iter=5, cv=3, verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUadgwFbQgoY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "best_params = rnd_search_cv.best_params_\n",
        "with open(\"rnd_search_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vy8Mt2bJ1NZ"
      },
      "outputs": [],
      "source": [
        "with open(\"rnd_search_scikeras.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rnd_search_cv, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0yULehWOv2I"
      },
      "source": [
        "Here we output the best combination of hyperparameters discovered by `RandomizedSearchCV`, which achieved the lowest validation loss during the search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQQG1alDQy3L"
      },
      "outputs": [],
      "source": [
        "rnd_search_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3gKeBKFXQrm"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of the Hyperparameter Tuning Code\n",
        "\n",
        "This code performs hyperparameter tuning for a neural network regression model using **Keras Tuner**.\n",
        "\n",
        "- **Model building function (`build_model_kt`)**:  \n",
        "  Defines a Keras Sequential model with a variable number of hidden layers (`n_hidden`) and neurons per layer (`n_neurons`).  \n",
        "  The learning rate and optimizer type (`adam`, `sgd`, `nesterov`) are also tuned.\n",
        "\n",
        "- **Keras Tuner Random Search**:  \n",
        "  Initializes a tuner that tries up to 20 different hyperparameter configurations to minimize validation mean squared error (`val_mse`).  \n",
        "  Uses callbacks:  \n",
        "  - EarlyStopping (stops training early if no improvement)  \n",
        "  - TensorBoard (for visualization)\n",
        "\n",
        "- **Running the tuner**:  \n",
        "  Trains models on the training data (`X_train`, `y_train`) with validation on (`X_valid`, `y_valid`), searching for the best hyperparameters.\n",
        "\n",
        "- **Saving results**:  \n",
        "  After tuning, the best hyperparameters are printed and saved to a pickle file.  \n",
        "  The best model is saved in Keras format for later use.\n",
        "\n",
        "---\n",
        "\n",
        "This approach helps find the optimal neural network architecture and training settings for regression tasks automatically.\n"
      ],
      "metadata": {
        "id": "wk1LOveLaaF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rd1e0giTJPg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "def build_model_kt(hp):\n",
        "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=3, default=2)\n",
        "    n_neurons = hp.Int(\"n_neurons\", min_value=1, max_value=100, step=1)\n",
        "    learning_rate = hp.Float(\"learning_rate\", min_value=3e-4, max_value=3e-2, sampling=\"log\")\n",
        "    optimizer_name = hp.Choice(\"optimizer\", values=[\"adam\", \"sgd\", \"nesterov\"])\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.InputLayer(input_shape=[8]))\n",
        "\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "\n",
        "    model.add(keras.layers.Dense(1))\n",
        "\n",
        "    if optimizer_name == \"adam\":\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"sgd\":\n",
        "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"nesterov\":\n",
        "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
        "    return model\n",
        "\n",
        "# setting up catalouge for tensorbaord\n",
        "project_name = \"my_california_housing\"\n",
        "directory = \"my_california_housing_dir\"\n",
        "\n",
        "# random search tuner initialization\n",
        "random_search_tuner = kt.RandomSearch(\n",
        "    build_model_kt,\n",
        "    objective=\"val_mse\",\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory=directory,\n",
        "    project_name=project_name,\n",
        "    overwrite=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Callback = EarlyStopping and TensorBoard\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
        "tensorboard_logdir = os.path.join(directory, project_name, \"tensorboard_logs\")\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(tensorboard_logdir)\n",
        "\n",
        "# we start tuning parameters\n",
        "random_search_tuner.search(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks=[early_stopping_cb, tensorboard_cb],\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "# we show the best parameters\n",
        "best_hp = random_search_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Najlepsze hiperparametry:\")\n",
        "print(f\"n_hidden: {best_hp.get('n_hidden')}\")\n",
        "print(f\"n_neurons: {best_hp.get('n_neurons')}\")\n",
        "print(f\"learning_rate: {best_hp.get('learning_rate')}\")\n",
        "print(f\"optimizer: {best_hp.get('optimizer')}\")\n",
        "\n",
        "# saving best parameters\n",
        "best_hp_dict = {\n",
        "    \"n_hidden\": best_hp.get(\"n_hidden\"),\n",
        "    \"n_neurons\": best_hp.get(\"n_neurons\"),\n",
        "    \"learning_rate\": best_hp.get(\"learning_rate\"),\n",
        "    \"optimizer\": best_hp.get(\"optimizer\"),\n",
        "}\n",
        "with open(\"kt_search_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_hp_dict, f)\n",
        "\n",
        "# build and save best model\n",
        "best_model = random_search_tuner.get_best_models(num_models=1)[0]\n",
        "best_model.save(\"kt_best_model.keras\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}