# -*- coding: utf-8 -*-
"""data_dimension_reduction_with_PCA_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17TVBT8SrC0pKpc0xtY53ENuvHMMM04_G

# Data Dimension reduction with PCA models

This project demonstrates dimensionality reduction using **Principal Component Analysis (PCA)** and **Locally Linear Embedding (LLE)** on two classic datasets:  
- `breast_cancer` (30 features)  
- `iris` (4 features)

## 1. Data import
"""

from sklearn import datasets

data_breast_cancer = datasets.load_breast_cancer(as_frame=True)
print(data_breast_cancer['DESCR'])

df_data_breast_cancer = data_breast_cancer.frame
df_data_breast_cancer.head(10)

data_iris = datasets.load_iris(as_frame=True)
print(data_iris['DESCR'])

df_data_iris = data_iris.frame
df_data_iris.head(10)

"""## 2. Data dimension reduction

**PCA is used to reduce the number of features while preserving at least 90% of the total variance.  
This helps simplify the data structure without losing essential information.**

**Variance** measures how spread out the values in a dataset are.  
In PCA, it represents how much of the data’s information is captured by each principal component.

Mathematically, variance is defined as:

$$
\text{Var}(X) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$

Where:
- $$ x_i $$ is a single data point,
- $$ \bar{x} $$ is the mean of the dataset,
- $$ n $$ is the number of samples.

Higher variance means more information is captured along that direction in the feature space.

**reduction of unscaled data**
"""

from sklearn.model_selection import train_test_split
X_bc = df_data_breast_cancer.iloc[:,:-1]
y_bc = df_data_breast_cancer['target']
X_bc_train, X_bc_test, y_bc_train, y_bc_test = train_test_split(X_bc, y_bc, test_size=0.2, shuffle = True)

from sklearn.decomposition import PCA

# for breast cancer data

pca_bc = PCA(n_components = 0.90) # and automatically centers data
X_bc_reduced = pca_bc.fit_transform(X_bc)

print("***BREAST CANCER DATA***")
print(f"Original data size: {X_bc.shape}:")
print(f"Transformed data size: {X_bc_reduced.shape}:")
print("variance's:", pca_bc.explained_variance_ratio_)
print("Sum of variances:", sum(pca_bc.explained_variance_ratio_))

from sklearn.model_selection import train_test_split
X_ir = df_data_iris.iloc[:,:-1]
y_ir = df_data_iris['target']
# y_ir = (y_ir == 2).astype(int)
X_ir_train, X_ir_test, y_ir_train, y_ir_test = train_test_split(X_ir, y_ir, test_size=0.2, shuffle = True)

# for iris data

pca_ir = PCA(n_components = 0.9) # and automatically centers data
X_ir_reduced = pca_ir.fit_transform(X_ir)

print("***IRIS DATA***")
print(f"Original data size: {X_ir.shape}:")
print(f"Transformed data size: {X_ir_reduced.shape}:")
print("variance's:", pca_ir.explained_variance_ratio_)
print("Sum of variances:", sum(pca_ir.explained_variance_ratio_))

"""**reduction of scaled data**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_bc_scaled = scaler.fit_transform(X_bc)

pca_bc_scaled = PCA(n_components = 0.90) # and automatically centers data
X_bc_scaled_reduced = pca_bc_scaled.fit_transform(X_bc_scaled)

print("***BREAST CANCER DATA SCALED***")
print(f"Original data size: {X_bc_scaled.shape}:")
print(f"Transformed data size: {X_bc_scaled_reduced.shape}:")
print("variance's:", pca_bc_scaled.explained_variance_ratio_)
print("Sum of variances:", sum(pca_bc_scaled.explained_variance_ratio_))

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_ir_scaled = scaler.fit_transform(X_ir)

pca_ir_scaled = PCA(n_components = 0.90) # and automatically centers data
X_ir_scaled_reduced = pca_ir_scaled.fit_transform(X_ir_scaled)

print("***IRIS DATA SCALED***")
print(f"Original data size: {X_ir_scaled.shape}:")
print(f"Transformed data size: {X_ir_scaled_reduced.shape}:")
print("variance's:", pca_ir_scaled.explained_variance_ratio_)
print("Sum of variances:", sum(pca_ir_scaled.explained_variance_ratio_))

"""**We apply `StandardScaler` to normalize all features to the same scale (mean = 0, standard deviation = 1).  
Without scaling, PCA may overemphasize features with larger numeric ranges, which can distort the `explained_variance_ratio_`.  
As shown in this analysis, scaling is essential to ensure that dimensionality reduction is accurate and meaningful.**
"""

import pickle
# Save explained variance ratios from scaled PCA

with open("pca_bc.pkl", "wb") as f:
    pickle.dump(list(pca_bc_scaled.explained_variance_ratio_), f)

with open("pca_ir.pkl", "wb") as f:
    pickle.dump(list(pca_ir_scaled.explained_variance_ratio_), f)

"""## 3. Finding the most decisive features

**Compute importance of original features and save index list**
"""

import numpy as np

def get_sorted_feature_indices(pca):
    weights = np.abs(pca.components_ * pca.explained_variance_ratio_[:, np.newaxis])
    sorted_indices = np.argsort(np.max(weights, axis=0))[::-1]
    return sorted_indices

# calculating the indexes for breast cancer dataset
idx_bc = get_sorted_feature_indices(pca_bc_scaled)
with open("idx_bc.pkl", "wb") as f:
    pickle.dump(idx_bc, f)

# calculating the indexes for iris dataset
idx_ir = get_sorted_feature_indices(pca_ir_scaled)
with open("idx_ir.pkl", "wb") as f:
    pickle.dump(idx_ir, f)

"""**Indexes of the most decisive features shown:**"""

print(idx_bc)
print(idx_ir)

print("bc explained var:", sum(pca_bc_scaled.explained_variance_ratio_))
print("ir explained var:", sum(pca_ir_scaled.explained_variance_ratio_))

"""**Scaling ensures that PCA treats all features equally.  
This improves component selection, makes variance ratios more meaningful, and leads to better dimensionality reduction results.**

**Dimensionality reduction is a powerful technique — it allows us to significantly compress the dataset while preserving most of its information.  
This not only speeds up training and inference in machine learning models, but also reduces memory usage and data storage requirements.**
"""